#!/bin/python

import math
import scipy.optimize as opt
import numpy as np
import argparse

np.seterr(all='raise')

import pickle
import sys

import astropy.units as u
from astropy import wcs
from astropy.io import fits
from astropy import table

from sklearn.neighbors import KDTree

import dithering_print_offsets

global grid_sampling

def calculate_integral(x_pos, y_pos, xo, yo, amp, sigma):
    max_x = max(x_pos, xo)+100
    min_x = min(x_pos, xo)-100
    max_y = max(y_pos, yo)+100
    min_y = min(y_pos, yo)-100
    xs = np.linspace(min_x, max_x, grid_sampling)
    ys = np.linspace(min_y, max_y, grid_sampling)
    dx = xs[1]-xs[0]
    dy = ys[1]-ys[0]
    xv, yv = np.meshgrid(xs, ys)
    integral = 0
    for i in range(grid_sampling):
        for j in range(grid_sampling):
            integral += integrand(xv[i,j], yv[i,j], x_pos, y_pos, amp, xo, yo, sigma)*dx*dy
    return integral

def integrand(x, y, x_offset, y_offset, amp, xo, yo, sigma):
    condition1 = (np.sqrt((x-x_offset)**2 + (y-y_offset)**2)) <=sigma
    condition2 = (np.sqrt((x-xo)**2       + (y-yo)**2)) <=sigma
    if condition1 and condition2:
        return twoDGaussian(x, y, amp, xo, yo, sigma, sigma)
    else:
        return 0

def twoDGaussian(x, y, amp, xo, yo, sigma_1, sigma_2, phi=0.0):
    A = (np.cos(phi) / sigma_1)**2 + (np.sin(phi) / sigma_2)**2
    B = (np.sin(phi) / sigma_1)**2 + (np.cos(phi) / sigma_2)**2
    C = 2 * np.sin(phi) * np.cos(phi)*(1/sigma_1**2 - 1/sigma_2**2)
    return amp * np.exp( -0.5 * ( A * (x-xo)**2 +
                                  B * (y-yo)**2 +
                                  C*(x-xo)*(y-yo) ) )

sigma_limits = [-5., 5.]
# this function yields the residual for the number of equations
def system(x, b):
    return (SOE(x) - b)

# SOE stands for system of equations
def SOE(x):
    calculated_integrals = np.zeros(n_equations)
    for idx in range(n_fibers):
        for exp_idx in range(n_exposures):
            eqn_idx = idx*n_exposures + exp_idx
            curr_sigma = x[idx*4+3] + x[n_fibers*4+exp_idx]
            try:
                curr_amp   = calculate_integral(curr_xs[eqn_idx], curr_ys[eqn_idx], x[idx*4+0], x[idx*4+1], x[idx*4+2], x[idx*4+3]) / \
                    (calculate_integral(curr_xs[eqn_idx], curr_ys[eqn_idx], x[idx*4+0], x[idx*4+1], x[idx*4+2], curr_sigma) / x[idx*4+2])
            except FloatingPointError:
                curr_amp = x[idx*4+2]
            
            if x[idx*4+2] < 0.001:
                calculated_integral = 1e12
            elif curr_sigma < 30. or curr_sigma > 90.:
                calculated_integral = 1e12
            elif x[n_fibers*4+exp_idx] < -5 or x[n_fibers*4+exp_idx] > 5.:
                calculated_integral = 1e12
            else:
                calculated_integrals[eqn_idx] = calculate_integral(curr_xs[eqn_idx], curr_ys[eqn_idx],
                                                                   x[idx*4+0], x[idx*4+1], curr_amp, curr_sigma)
    return calculated_integrals
    
def proper_fit_vecfunc(inputs):
    if not np.asarray(inputs).all():
        print("Signals       : ", curr_signals)
    num_dithering_points = len(curr_xs)
    amplitude = inputs[0]
    xo        = inputs[1]
    yo        = inputs[2]
    sigma     = inputs[3]
    sigmas    = inputs[4:]
    F         = np.empty((num_dithering_points+4))
    for iDither in range(num_dithering_points):
        condition = 0
        curr_sigma = sigma + sigmas[iDither]
        if (curr_sigma < 30.) or (curr_sigma > 70.):
            condition = 1
        if (sigmas[iDither] < sigma_limits[0]) or (sigmas[iDither] > sigma_limits[1]):
            condition = 2
        if (amplitude < 1e-3):
            condition = 3
        if condition == 0:
            F[iDither] = np.sqrt(np.fabs(curr_signals[iDither]**2 -
                                         (calculate_integral(curr_xs[iDither], curr_ys[iDither], xo, yo, amplitude, curr_sigma))**2))
        else:
            F[iDither] = 1e12
    for iDummy in range(4):
        F[num_dithering_points+iDummy] = 1e-12
    return F

def run_analysis(pattern, search_radius1, search_radius2, search_radius3=None, template=1, seeing_rms=0.00001):
    filenames = []
    # two files to be combine by default
    # in case a third search radius is defined, add it to the list as well
    filenames.append("{}/{:.1f}um/result_{}_seeing{}_blur_wlenoffset-randoffset{}_combined_template{}.fits".format(data_path,
                                                                                                                   search_radius1,
                                                                                                                   pattern,
                                                                                                                   seeing_rms,
                                                                                                                   template,
                                                                                                                   template))
    filenames.append("{}/{:.1f}um/result_{}_seeing{}_blur_wlenoffset-randoffset{}_combined_template{}.fits".format(data_path,
                                                                                                                   search_radius2,
                                                                                                                   pattern,
                                                                                                                   seeing_rms,
                                                                                                                   template,
                                                                                                                   template))
    try:
        filenames.append("{}/{:.1f}um/result_{}_seeing{}_blur_wlenoffset-randoffset{}_combined_template{}.fits".format(data_path,
                                                                                                                       search_radius3,
                                                                                                                       pattern,
                                                                                                                       seeing_rms,
                                                                                                                       template,
                                                                                                                       template))
    except:
        pass

    print(filenames)
    
    dithering_pos_xs = []
    dithering_pos_ys = []
    calc_signals = []
    focal_xs     = []
    focal_ys     = []
    mags         = []
    
    # tables corresponding to the files
    for filename in filenames:
        hdus = fits.open(filename)
        dithering_pos_xs.append(hdus[1].data['dither_pos_x'])
        dithering_pos_ys.append(hdus[1].data['dither_pos_y'])
        calc_signals.append(hdus[1].data['calc_signals'])
        focal_xs.append(hdus[1].data['focal_x'])
        focal_ys.append(hdus[1].data['focal_y'])
        mags.append(hdus[1].data['mag'])

    focal_coordinates = np.vstack((focal_xs[0], focal_ys[0])).T
    tree = KDTree(focal_coordinates, leaf_size=4)
        
    input_focal_x    = []
    input_focal_y    = []
    printed_offsets_x= []
    printed_offsets_y= []
    calc_offsets_x   = []
    calc_offsets_y   = []
    calc_sigma       = []
    calc_delta_sigma = []
    calc_amplitude   = []
    calc_convergence = []
    calc_cost        = []
    #for iFiber in range(len(dithering_pos_xs[0])):
    for iFiber in range(start_idx, end_idx):

        global curr_xs, curr_ys, curr_signals
        global neighbors, n_fibers, n_exposures
        global n_equations, n_unknowns

        curr_focal_x = focal_xs[0][iFiber]
        curr_focal_y = focal_ys[0][iFiber]
        neighbors = tree.query_radius(focal_coordinates[iFiber].reshape(1, 2), r=10.0)[0] # this is optional for now
        neighbors = np.hstack(([iFiber], neighbors[np.where(neighbors!=iFiber)]))
        print(neighbors)
        n_fibers = len(neighbors)
        #if n_fibers == 1:
        #    n_fibers += 1
        n_exposures = 9 * len(dithering_pos_xs)

        # add the original fiber's data
        curr_xs = dithering_pos_xs[0][iFiber]
        curr_ys = dithering_pos_ys[0][iFiber]
        curr_signals = calc_signals[0][iFiber]

        #print(len(dithering_pos_xs))
        
        for iSet, _ in enumerate(dithering_pos_xs):
            if iSet == 0:
                continue
            focal_x_indices = (focal_xs[iSet] == curr_focal_x)
            focal_y_indices = (focal_ys[iSet] == curr_focal_y)
            index        = np.logical_and(focal_x_indices, focal_y_indices)
            ifiber_match = np.where(index)[0]
            #print(curr_focal_x, curr_focal_y)
            #print(focal_xs[iSet][ifiber_match], focal_ys[iSet][ifiber_match])
            # just a sanity check
            if ( (curr_focal_x == focal_xs[iSet][ifiber_match]) and (curr_focal_y == focal_ys[iSet][ifiber_match]) ):
                curr_xs = np.concatenate((curr_xs, dithering_pos_xs[iSet][ifiber_match]), axis=None)
                curr_ys = np.concatenate((curr_ys, dithering_pos_ys[iSet][ifiber_match]), axis=None)
                curr_signals = np.concatenate((curr_signals, calc_signals[iSet][ifiber_match]), axis=None)
            else:
                print("failed the sanity check")
        # add the original fiber's data
        for _, nbr_idx in enumerate(neighbors):
            if nbr_idx == iFiber:# and n_fibers != 1:
                continue
            nbr_focal_x = focal_xs[0][nbr_idx]
            nbr_focal_y = focal_ys[0][nbr_idx]
            curr_xs = np.concatenate((curr_xs, dithering_pos_xs[0][nbr_idx]), axis=None)
            curr_ys = np.concatenate((curr_ys, dithering_pos_ys[0][nbr_idx]), axis=None)
            curr_signals = np.concatenate((curr_signals, calc_signals[0][nbr_idx]), axis=None)
            for iSet, _ in enumerate(dithering_pos_xs):
                if iSet == 0:
                    continue
                focal_x_indices = (focal_xs[iSet] == nbr_focal_x)
                focal_y_indices = (focal_ys[iSet] == nbr_focal_y)
                index        = np.logical_and(focal_x_indices, focal_y_indices)
                ifiber_match = np.where(index)[0]
                # just a sanity check
                if ( (nbr_focal_x == focal_xs[iSet][ifiber_match]) and (nbr_focal_y == focal_ys[iSet][ifiber_match]) ):
                    curr_xs = np.concatenate((curr_xs, dithering_pos_xs[iSet][ifiber_match]), axis=None)
                    curr_ys = np.concatenate((curr_ys, dithering_pos_ys[iSet][ifiber_match]), axis=None)
                    curr_signals = np.concatenate((curr_signals, calc_signals[iSet][ifiber_match]), axis=None)
                else:
                    print("failed the sanity check")

        n_equations = len(curr_xs)
        n_unknowns = 4*n_fibers+n_exposures

        print("- Fiber ID:            {}".format(iFiber))
        print("- Number of neighbors: {}".format(len(neighbors)))
        print("-- Neighbors:          ", neighbors)
        print("- Number of fibers:    {}".format(n_fibers))
        print("- Number of exposures: {}".format(n_exposures))
        print("- Number of unknowns:  {}".format(n_unknowns))
        print("- Number of equations: {}".format(n_equations))

        curr_xs = np.asarray(curr_xs).flatten()
        curr_ys = np.asarray(curr_ys).flatten()
        curr_signals  = np.asarray(curr_signals).flatten()
        initial_guess = np.zeros(n_equations)

        try:
            for idx in range(n_fibers):
                start_ = idx*n_exposures
                end_   = (idx+1)*n_exposures
                x_guess = np.sum(curr_xs[start_:end_] * curr_signals[start_:end_])/np.sum(curr_signals[start_:end_])
                y_guess = np.sum(curr_ys[start_:end_] * curr_signals[start_:end_])/np.sum(curr_signals[start_:end_])
                initial_guess[0+idx*4] = x_guess
                initial_guess[1+idx*4] = y_guess
                initial_guess[2+idx*4] = np.random.uniform(0.005, 0.02)
                initial_guess[3+idx*4] = np.random.uniform(40., 60.)
                signal_guess           = calculate_integral(curr_xs[start_], curr_ys[start_], \
                                                            initial_guess[0+idx*4], initial_guess[1+idx*4], \
                                                            initial_guess[2+idx*4], initial_guess[3+idx*4])
                scaling_factor         = signal_guess/curr_signals[start_]
                initial_guess[2+idx*4] /= scaling_factor*np.random.uniform(0.85, 1.15)
            for idx in range(n_exposures):
                initial_guess[n_fibers*4+idx] = np.random.uniform(-5., 5.)

            print("- Initial guess: \n", initial_guess[:n_unknowns])
            print("- Signals with initial guess: \n", SOE(initial_guess))
            print("- Signals (measured): \n", curr_signals)

            solution = opt.root(system, initial_guess, args=curr_signals, method='lm', \
                                options={'factor': 100., 'xtol': 1e-9, 'eps': 1., 'diag': 20*np.ones(n_equations)})
            #solution = opt.root(system, solution.x, args=curr_signals, method='lm', options={'factor': 75., 'xtol': 5e-9})
            solution_msg = solution.message
            solution_ier = solution.success
            solution_fev = solution.nfev
            solution     = solution
            print("- Solution: \n", solution.x[:n_unknowns])
            print("- Relative errors for exposures: \n", 100*np.fabs(SOE(np.asarray(solution.x)) - curr_signals)/curr_signals)
            
            printed_offset = dithering_print_offsets.print_offset(curr_focal_x, curr_focal_y)
            print("Solver message: ", solution_msg)
            print("Convergence   : ", solution_ier)
            print("Num of F ev   : ", solution_fev)
            print("Printed offset: ", printed_offset[0], printed_offset[1])
            print("Solution      : A={} x={} y={}".format(solution.x[2], solution.x[0], solution.x[1]))
            print("              : PSF={}".format(solution.x[3]))
            print("              : delta(PSF)=", solution.x[4*n_fibers:4*n_fibers+n_exposures])
            
            # add the results
            input_focal_x.append(curr_focal_x)
            input_focal_y.append(curr_focal_y)
            printed_offsets_x.append(printed_offset[0])
            printed_offsets_y.append(printed_offset[1])
            calc_x_offset = solution.x[0]
            calc_y_offset = solution.x[1]
            calc_offsets_x.append(calc_x_offset)
            calc_offsets_y.append(calc_y_offset)
            calc_sigma.append(solution.x[3])
            calc_delta_sigma.append(solution.x[4*n_fibers:4*n_fibers+n_exposures])
            calc_convergence.append(solution.success)
            calc_amplitude.append(solution.x[2])
            calc_cost.append(None)
        except IndexError:
            continue
            
        del curr_xs, curr_ys, curr_signals
        
    calc_offsets_x = np.array(calc_offsets_x)
    calc_offsets_y = np.array(calc_offsets_y)

    return input_focal_x, input_focal_y, \
        calc_offsets_x, calc_offsets_y, calc_sigma, calc_delta_sigma, calc_amplitude, calc_convergence, calc_cost, \
        printed_offsets_x, printed_offsets_y
    #focal_xs[0], focal_ys[0], calc_offsets_x, calc_offsets_y, calc_sigma, calc_delta_sigma, calc_amplitude, calc_convergence, calc_cost

def run(search_radius1, search_radius2, search_radius3=None, start_idx=0, end_idx=1, template=1, prefix=None, grid_sampling=20, seeing_rms=0.00001):
    
    # calculate the values for a combination of search radii
    focal_x, focal_y, calc_offset_x, calc_offset_y, calc_sigma, \
        calc_delta_sigma, calc_amplitude, calc_convergence, calc_cost, \
        printed_offsets_x, printed_offsets_y = run_analysis(pattern, #"triangle", 
                                                            search_radius1,
                                                            search_radius2,
                                                            search_radius3,
                                                            template=template,
                                                            seeing_rms=seeing_rms)

    results_summary = {'focal_x': focal_x,
                       'focal_y': focal_y,
                       'calc_offset_x': calc_offset_x,
                       'calc_offset_y': calc_offset_y,
                       'calc_sigma': calc_sigma,
                       'calc_delta_sigma': calc_delta_sigma,
                       'calc_amplitude': calc_amplitude,
                       'calc_convergence': calc_convergence,
                       'calc_cost': calc_cost,
                       'printed_offset_x': printed_offsets_x,
                       'printed_offset_y': printed_offsets_y}
    
    pickle.dump(results_summary, open("results_{}_{}_summary_{}_{}_new.pkl".format(search_radius1, search_radius2, start_idx, end_idx), "wb"))

if __name__ == "__main__":
    parser = argparse.ArgumentParser("Script to combine and analyze different search radii")
    parser.add_argument("--stepsize", required=True, nargs="+", type=float)
    parser.add_argument("--template",  required=True, type=int)
    parser.add_argument("--prefix",    required=True, type=str)
    parser.add_argument("--sampling", type=int, default=6)
    parser.add_argument("--start_idx", type=int, required=True)
    parser.add_argument("--end_idx", type=int, required=True)
    parser.add_argument("--pattern", type=str, default="triangle", choices=('triangle', 'rectangle'))
    parser.add_argument("--seeing_rms", type=str, required=False, default="0.00001")
    parsed_args = parser.parse_args()

    data_path     = parsed_args.prefix
    stepsizes     = np.asarray(parsed_args.stepsize)
    template      = parsed_args.template
    prefix        = parsed_args.prefix
    grid_sampling = parsed_args.sampling
    start_idx     = parsed_args.start_idx
    end_idx       = parsed_args.end_idx
    seeing_rms    = parsed_args.seeing_rms
    global pattern
    pattern       = parsed_args.pattern
    
    # load the predefined offset field interpolator
    #pickled_data = pickle.load(open("data/offset_field{}_CT2D.pkl".format(template), "rb"))
    #xs = np.array(pickled_data["x"])
    #ys = np.array(pickled_data["y"])
    #xys = list(zip(xs, ys))
    #interpolator_org_x = pickled_data["interpolator_x"]
    #interpolator_org_y = pickled_data["interpolator_y"]
    
    stepsize = [None, None, None]
    for i, _ in enumerate(stepsizes):
        stepsize[i] = stepsizes[i]
    run(*stepsize, start_idx=start_idx, end_idx=end_idx, template=template, prefix=prefix, grid_sampling=grid_sampling, seeing_rms=seeing_rms)

